{
	"nodes":[
		{"id":"a92a0e860be59684","type":"text","text":"## Funciones de activacion\n\n#programming \n\nLas funciones de activación son componentes cruciales en las redes neuronales artificiales. Aquí se describen algunas de sus principales utilidades:\n\n1. **Introducir No Linealidad**: Sin funciones de activación, una red neuronal sería simplemente una combinación lineal de sus entradas, lo que limitaría su capacidad para modelar relaciones complejas. Las funciones de activación introducen no linealidad, permitiendo que la red neuronal aprenda y represente patrones complejos.\n    \n2. **Controlar la Salida de Neuronas**: Las funciones de activación determinan si una neurona debe activarse o no, basándose en la suma ponderada de sus entradas. Esto ayuda a la red a decidir qué información es relevante y debe ser pasada a la siguiente capa.\n    \n3. **Normalización de Salidas**: Algunas funciones de activación, como la función sigmoide y la tangente hiperbólica (tanh), mapean la salida de una neurona a un rango específico. Esto puede ayudar a estabilizar el aprendizaje y a controlar las explosiones de gradiente.\n    \n4. **Mejorar la Convergencia del Entrenamiento**: Funciones de activación adecuadas pueden acelerar la convergencia del proceso de entrenamiento de la red neuronal al facilitar la propagación del error y la actualización de los pesos.","x":-527,"y":-1284,"width":795,"height":634},
		{"id":"0d2af9c3a0b623a8","type":"text","text":"## Desvanecimiento del gradiente\n\n#glosario\n\nEl problema del desvanecimiento del gradiente (vanishing gradient problem) es un desafío común en el entrenamiento de redes neuronales profundas. Este problema ocurre cuando los gradientes de las funciones de pérdida con respecto a los parámetros (pesos) de las capas anteriores se vuelven extremadamente pequeños, dificultando la actualización efectiva de los pesos durante el entrenamiento. A continuación se presentan los detalles del problema y sus causas:\n\n### Causas del Desvanecimiento del Gradiente\n\n1. **Funciones de Activación**: Funciones de activación como la sigmoide y `tanh` comprimen su rango de salida a valores entre 0 y 1 (o -1 y 1 en el caso de `tanh`). Cuando los gradientes se calculan a través de muchas capas, estos valores pequeños se multiplican repetidamente, lo que reduce exponencialmente el gradiente a medida que se propaga hacia atrás a través de la red.\n    \n2. **Inicialización de Pesos**: La inicialización inapropiada de los pesos puede causar que los gradientes sean demasiado pequeños o demasiado grandes. Inicializar pesos con valores muy pequeños puede exacerbar el problema del desvanecimiento del gradiente.\n    \n\n### Consecuencias del Desvanecimiento del Gradiente\n\n- **Entrenamiento Lento o Estancado**: Cuando los gradientes son muy pequeños, las actualizaciones de los pesos durante el entrenamiento son mínimas. Esto puede llevar a un entrenamiento extremadamente lento o incluso a que el modelo no aprenda en absoluto.\n- **Capas Inferiores No Aprenden**: Las capas más cercanas a la entrada de la red pueden recibir gradientes tan pequeños que prácticamente no se actualizan, lo que significa que no aprenden características útiles.","x":-527,"y":-2120,"width":795,"height":780,"color":"6"},
		{"id":"62e3feec0cb5d95d","type":"text","text":"  \n## Función ReLU\n\n[#programming](app://obsidian.md/index.html#programming)\n\nLa función de activación ReLU (Rectified Linear Unit) es una de las funciones de activación más populares y ampliamente utilizadas en redes neuronales, especialmente en redes profundas. Su fórmula es bastante simple:\n\nReLU(x)=max(0,x)\n\nEsto significa que para cualquier entrada x, la salida es x si x es mayor que 0, y 0 si x es menor o igual a 0.\n\n### Características de ReLU:\n\n1. **Simplicidad Computacional**: La operación de ReLU es muy rápida y simple de calcular.\n2. **Soluciona el Problema del Desvanecimiento del Gradiente**: A diferencia de funciones de activación sigmoide o tanh, ReLU no satura en la región positiva, lo que ayuda a mitigar el problema del desvanecimiento del gradiente, permitiendo que las redes profundas se entrenen más eficientemente.\n3. **Sparsity**: Debido a que la salida de ReLU es 0 para todas las entradas negativas, introduce sparsity en la red, lo que puede llevar a una mayor eficiencia computacional y mejor representatividad.\n\n```python\nimport numpy as np\n\ndef relu(x): \n\treturn np.maximum(0, x) \n\n# Ejemplo de uso \n\ninput_data = np.array([-2, -1, 0, 1, 2]) \noutput_data = relu(input_data) \nprint(\"Input:\", input_data) \nprint(\"Output:\", output_data)\n```","x":-527,"y":-580,"width":795,"height":874},
		{"id":"fd0847631d1a96ac","type":"text","text":"## Función Sigmoide\n\n#programming \n\nLa función sigmoide es una de las funciones de activación más antiguas y ampliamente utilizadas en redes neuronales, especialmente en los primeros días del aprendizaje automático. Su fórmula matemática es:\n![[Pasted image 20250131123546.png]]\n\n### Características de la Función Sigmoide\n\n1. **Rango de Salida**: La función sigmoide mapea cualquier valor de entrada a un rango entre 0 y 1. Esto es útil en problemas donde necesitamos una salida que represente una probabilidad.\n    \n2. **Suavidad**: La función sigmoide es una función suave y diferenciable en todos los puntos, lo que permite calcular gradientes necesarios para la optimización durante el entrenamiento de la red neuronal.\n    \n3. **Forma S**: La curva de la función sigmoide tiene forma de S, lo cual le permite comprimir valores de entrada grandes y pequeños en el rango de 0 a 1.\n    \n\n### Ventajas\n\n- **Interpretabilidad**: Dado que la salida está en el rango [0, 1], es ideal para aplicaciones de clasificación binaria, donde la salida puede ser interpretada como una probabilidad.\n- **Diferenciabilidad**: La función sigmoide es diferenciable, lo cual es esencial para el cálculo de gradientes durante el entrenamiento con retropropagación.\n\n### Desventajas\n\n- **Desvanecimiento del Gradiente**: En las regiones donde la entrada es muy grande o muy pequeña, la derivada de la función sigmoide se acerca a cero, lo que puede causar el problema del desvanecimiento del gradiente y dificultar el entrenamiento de redes profundas.\n- **No Cero-Centrada**: La salida de la función sigmoide no está centrada en cero, lo que puede afectar la dinámica del entrenamiento.\n\n### Aplicaciones Comunes\n\n- **Clasificación Binaria**: La función sigmoide se utiliza frecuentemente en la capa de salida de redes neuronales para problemas de clasificación binaria.\n- **Redes Neuronales Recurrentes (RNN)**: En algunas arquitecturas de RNN, la función sigmoide se utiliza en las puertas de control.\n\n```python\nimport numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Ejemplo de uso\ninput_data = np.array([-2, -1, 0, 1, 2])\noutput_data = sigmoid(input_data)\n\nprint(\"Input:\", input_data)\nprint(\"Output:\", output_data)\n```","x":-1920,"y":-1910,"width":1100,"height":1260},
		{"id":"b7a8414c3cfecc65","type":"text","text":"## Función Tanh\n\n#programming \n\nLa función de activación tangente hiperbólica, comúnmente conocida como `tanh`, es otra función de activación popular utilizada en redes neuronales. Su fórmula matemática es:\n\n ![[Pasted image 20250131124824.png]]\n\n### Características de la Función tanh\n\n1. **Rango de Salida**: La función `tanh` mapea cualquier valor de entrada a un rango entre -1 y 1. Esto significa que la salida puede ser negativa, a diferencia de la función sigmoide.\n    \n2. **Forma S**: Similar a la función sigmoide, la curva de `tanh` tiene forma de S, pero está centrada en el origen (0,0).\n    \n3. **Simetría**: La función `tanh` es simétrica alrededor del origen, lo que significa que tanh⁡(−x)=−tanh⁡(x).\n\n### Ventajas\n\n- **Centrado en Cero**: A diferencia de la función sigmoide, la salida de `tanh` está centrada en cero, lo cual puede ayudar a que la convergencia sea más rápida y estable durante el entrenamiento de la red neuronal.\n- **Diferenciabilidad**: La función `tanh` es diferenciable, lo cual permite el cálculo de gradientes necesarios para la optimización durante el entrenamiento.\n\n### Desventajas\n\n- **Desvanecimiento del Gradiente**: Al igual que la función sigmoide, `tanh` puede sufrir del problema de desvanecimiento del gradiente, especialmente para valores de entrada muy grandes o muy pequeños, donde la derivada se aproxima a cero.\n\n### Aplicaciones Comunes\n\n- **Redes Neuronales Recurrentes (RNN)**: La función `tanh` se utiliza frecuentemente en redes neuronales recurrentes y en las capas ocultas de otros modelos de redes neuronales debido a su capacidad para modelar relaciones no lineales de manera efectiva.\n- **Problemas de Clasificación**: Aunque es menos común que la función sigmoide para la salida de clasificación binaria, `tanh` se puede usar en capas ocultas para ayudar a la red a aprender representaciones intermedias.\n\n```python\nimport numpy as np\n\ndef tanh(x):\n    return np.tanh(x)\n\n# Ejemplo de uso\ninput_data = np.array([-2, -1, 0, 1, 2])\noutput_data = tanh(input_data)\n\nprint(\"Input:\", input_data)\nprint(\"Output:\", output_data)import numpy as np\n```","x":480,"y":-1920,"width":1120,"height":1270}
	],
	"edges":[
		{"id":"6f011a9feb751fe5","fromNode":"a92a0e860be59684","fromSide":"right","toNode":"b7a8414c3cfecc65","toSide":"left"},
		{"id":"473dc96e8449b927","fromNode":"a92a0e860be59684","fromSide":"bottom","toNode":"62e3feec0cb5d95d","toSide":"top"},
		{"id":"2c323c5d243a3188","fromNode":"fd0847631d1a96ac","fromSide":"right","toNode":"a92a0e860be59684","toSide":"left"}
	]
}